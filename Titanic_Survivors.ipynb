{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use pandas library in python to read in the csv file. This creates a pandas dataframe and assigns it to the titanic variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000         NaN    0.000000   \n",
      "50%     446.000000    0.000000    3.000000         NaN    0.000000   \n",
      "75%     668.500000    1.000000    3.000000         NaN    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vanshdeep Sharma\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv('d:/titanic_train.csv')\n",
    "\n",
    "# print the first 5 rows of the dataframe.\n",
    "print(titanic['Age'].median())\n",
    "# .describe() gives the mean, standard deviation, min, max for all \n",
    "# columns that have numerical values\n",
    "print(titanic.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now have to convert our non-numeric columns (Name, Sex, Cabin, Embarked and Ticket) to numeric so as to make sense of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: Sex, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique genders -- the column appears to contain only male and female.\n",
    "print(titanic[\"Sex\"].unique())\n",
    "\n",
    "# Replace all the occurences of male with 0 and female with 1.\n",
    "titanic.loc[titanic['Sex'] == 'male', 'Sex'] = 0\n",
    "titanic.loc[titanic['Sex'] == 'female', 'Sex'] = 1\n",
    "print(titanic['Sex'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: Embarked, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now replace all the unique values in Embarked (S, C, and Q) with 0, 1, 2\n",
    "# Replace all nan with 'S'\n",
    "print(titanic['Embarked'].unique())\n",
    "titanic['Embarked'] = titanic['Embarked'].fillna('S')\n",
    "\n",
    "titanic.loc[titanic['Embarked'] == 'S', 'Embarked'] = 0\n",
    "titanic.loc[titanic['Embarked'] == 'C', 'Embarked'] = 1\n",
    "titanic.loc[titanic['Embarked'] == 'Q', 'Embarked'] = 2\n",
    "\n",
    "titanic['Embarked'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression \n",
    "- We use it for making close estimate predicitons. Linear regression follows the equation y =mx + b where y is the value we are trying to predict, m is a coefficient called the slope, x is the value of a column and b is a constant called the intercept.\n",
    "- This simple model can predict survival fairly well, hence linear regression can be a very powerful algorithm.\n",
    "\n",
    "### Cross Validation\n",
    "- We use cross validation to train the algorithm on different data than we make predictions on. This is critical if we want to avoid overfitting. To cross validate, you split your data into some number of parts ('folds'), let us assume 3 parts.\n",
    "- Combine the first two parts, train a model, make predictions on the third.\n",
    "- Combine the first and third parts, train the model, make predictions on the second\n",
    "- Combine the second and third parts, train the model, make predictions on the first\n",
    "- This way we generate predictions for the whole dataset without ever evaluating accuracy on the same data we train out model using.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  6.04646972e-02,   9.60063387e-01,   5.84639401e-01,\n",
      "         9.30785891e-01,   6.37667072e-02,   1.93126970e-01,\n",
      "         4.18587747e-01,   2.16701169e-02,   5.31894732e-01,\n",
      "         8.04998591e-01,   5.78039298e-01,   9.02044320e-01,\n",
      "         9.50519534e-02,  -9.50932900e-02,   6.09638639e-01,\n",
      "         7.15721816e-01,   5.10684222e-02,   3.27054646e-01,\n",
      "         5.48243209e-01,   6.66678021e-01,   2.59373023e-01,\n",
      "         2.56141421e-01,   6.63818886e-01,   4.66122886e-01,\n",
      "         5.11308692e-01,   4.09191082e-01,   1.64525347e-01,\n",
      "         4.35709259e-01,   6.95042779e-01,   1.36702544e-01,\n",
      "         4.66010001e-01,   1.07009313e+00,   6.94989933e-01,\n",
      "         1.88377007e-01,   4.83220770e-01,   4.13583512e-01,\n",
      "         1.64527065e-01,   9.29662703e-02,   5.45268417e-01,\n",
      "         6.09032704e-01,   5.25985149e-01,   7.46077379e-01,\n",
      "         1.64799719e-01,   7.80677042e-01,   6.55414800e-01,\n",
      "         1.36765615e-01,   1.65918507e-01,   6.94989933e-01,\n",
      "         1.10260088e-01,   5.75275285e-01,  -1.12340499e-02,\n",
      "         9.28640148e-02,   9.39348365e-01,   7.43951123e-01,\n",
      "         4.01894736e-01,   5.24522012e-01,   7.84385420e-01,\n",
      "         1.05085098e-01,   7.42752054e-01,  -7.27010094e-02,\n",
      "         1.18642038e-01,   9.65620205e-01,   4.20200429e-01,\n",
      "        -5.69529989e-03,   5.49437324e-01,   1.11731733e-01,\n",
      "         7.67699955e-01,   9.71819336e-02,   4.31084608e-01,\n",
      "         2.26110358e-02,   2.59290232e-01,   4.19023249e-01,\n",
      "         3.08001129e-01,   8.31658163e-02,   8.98391522e-02,\n",
      "         8.44599292e-02,   1.36702544e-01,   1.36765615e-01,\n",
      "         2.79896629e-01,   5.78157718e-01,   9.12691581e-02,\n",
      "         7.68738875e-02,   6.95005271e-01,   4.70867540e-01,\n",
      "         7.92728152e-01,   4.83015101e-01,   6.11663984e-03,\n",
      "         1.36765615e-01,   9.29519201e-01,   8.67092211e-02,\n",
      "         7.62808057e-02,   9.49718669e-02,   4.08993556e-01,\n",
      "         5.60093149e-03,   1.33830956e-02,   1.36765615e-01,\n",
      "         4.04189738e-01,   4.90057507e-01,   7.36398689e-01,\n",
      "         2.31370034e-01,   5.80456091e-01,   1.36702544e-01,\n",
      "         4.71829047e-01,   6.81852044e-02,  -6.33131770e-04,\n",
      "         7.83034176e-02,   5.94955335e-01,   1.36653134e-01,\n",
      "         5.74465867e-02,   6.71609221e-01,   4.33243769e-01,\n",
      "         6.09303845e-01,   9.08805872e-02,   5.67841968e-01,\n",
      "         6.34179987e-01,   9.29151426e-02,   4.57966022e-02,\n",
      "         2.39753340e-01,   5.63298330e-01,   4.71920516e-01,\n",
      "         2.47823784e-01,   1.36765615e-01,   2.64260781e-01,\n",
      "         7.61422619e-01,   4.03001505e-01,   1.11051396e-01,\n",
      "         1.92837260e-01,   8.63377064e-02,   6.16793575e-01,\n",
      "         4.24701777e-02,   9.59721777e-02,   9.46429315e-02,\n",
      "         5.13440703e-01,   7.43951123e-01,   2.74912569e-01,\n",
      "         3.08017888e-01,   9.31305623e-01,   4.24461852e-01,\n",
      "         1.03871892e-01,   5.20437053e-01,   6.17987453e-01,\n",
      "         5.92910554e-01,   5.61963593e-01,   1.52800259e-01,\n",
      "         2.88898817e-01,   2.41066641e-01,   8.03481984e-02,\n",
      "         5.18766048e-01,   2.04273248e-01,   2.39455956e-01,\n",
      "         2.20490523e-01,   9.63421568e-01,   2.10102039e-02,\n",
      "         2.96239092e-03,   1.36463961e-01,   4.30848886e-01,\n",
      "         6.61612173e-01,   7.41951226e-02,   1.37016141e-01,\n",
      "        -1.30760166e-01,   2.23025607e-02,   7.46904806e-01,\n",
      "         8.24253739e-02,   1.01559529e-01,   1.28004857e-03,\n",
      "         7.11257653e-02,   1.00866499e+00,   4.19150461e-01,\n",
      "         5.20605627e-01,   9.81818845e-02,   3.96477300e-01,\n",
      "         4.68970560e-02,   5.82019445e-01,   9.29151426e-02,\n",
      "         4.33855912e-01,   4.30689351e-02,   2.76377848e-02,\n",
      "         9.47711470e-01,   2.64484153e-01,   5.83883973e-02,\n",
      "         3.71392508e-01,   3.55990316e-01,  -4.47859237e-02,\n",
      "         2.49440563e-01,   5.84320387e-01,   5.30452830e-01,\n",
      "         6.68071181e-01,   4.27005527e-01,   5.65055584e-02,\n",
      "         6.16179529e-02,   7.62465461e-01,   2.87426667e-01,\n",
      "         5.69121551e-01,   2.70040584e-01,   9.59819942e-01,\n",
      "         9.79212187e-01,   1.92837260e-01,   2.33261757e-02,\n",
      "         6.94989933e-01,   7.79150925e-01,   7.89595706e-02,\n",
      "        -1.30760166e-01,   6.52166883e-02,   6.96267672e-02,\n",
      "         9.92233196e-02,   6.09748062e-01,   4.31254551e-02,\n",
      "         1.15026904e-01,   6.61619004e-01,   4.67351265e-01,\n",
      "         8.63001991e-02,   7.59480587e-01,   9.05533697e-02,\n",
      "         2.64484153e-01,   1.62748587e-01,   9.91838695e-01,\n",
      "         5.82553718e-01,   2.15093591e-01,   1.00471470e+00,\n",
      "         2.63461598e-01,   1.03394686e-01,   2.70741203e-01,\n",
      "         3.03957778e-02,   1.36702544e-01,   4.37469079e-01,\n",
      "         9.14123158e-02,   2.86404112e-01,   9.36818943e-02,\n",
      "         2.89512350e-01,   5.29790458e-01,   9.43209933e-01,\n",
      "         7.61683246e-02,   2.04203855e-01,   4.65709482e-01,\n",
      "         2.75975697e-01,   6.38713778e-01,   2.10513203e-01,\n",
      "         7.65970145e-01,   2.86404112e-01,   2.57930563e-01,\n",
      "         6.39546250e-01,   6.68071181e-01,   2.65547282e-01,\n",
      "         9.05022419e-02,   1.01954855e-01,   4.51060659e-01,\n",
      "         5.86663730e-01,   7.27793207e-01,   3.98252982e-01,\n",
      "         1.89656372e-01,   1.36438398e-01,   5.23345947e-01,\n",
      "         3.91548914e-01,   4.73990768e-02,   5.06408761e-01,\n",
      "         5.57502644e-01,   1.07264612e+00,   9.84964313e-01,\n",
      "         1.17680656e+00,   7.04254825e-01,   1.92837260e-01,\n",
      "        -3.22718252e-02,   3.78050513e-01,   4.26574409e-01,\n",
      "         6.94989933e-01,   2.50947500e-01,  -3.00051977e-02,\n",
      "         5.44223844e-02,   9.27968693e-01,   9.94632496e-01,\n",
      "         5.22681413e-01,   8.13309113e-02,   7.20367330e-01,\n",
      "         4.47090961e-01,   6.94989933e-01,   8.82554356e-01,\n",
      "         5.44939843e-01,   3.21737360e-01,   4.06400067e-02,\n",
      "         5.14835151e-01,   5.72678591e-02,   7.82864022e-02,\n",
      "         1.03987768e-01,   9.71376365e-02,   5.20636303e-01,\n",
      "         9.62857748e-02,   7.47882044e-02,   9.08175161e-02,\n",
      "         2.39455956e-01,   6.49104905e-01,   9.90178027e-01,\n",
      "         1.00778832e+00,   2.80016103e-01,   5.89189112e-01,\n",
      "         8.66461499e-02,   5.49437324e-01,   1.15513513e-01]), array([  1.04616312e+00,   4.98442385e-01,   1.02575476e+00,\n",
      "         7.27102602e-01,   1.35500962e-01,   1.07474736e-01,\n",
      "         8.80030809e-01,   1.81533670e-01,   5.40182875e-01,\n",
      "         1.09330669e+00,   9.84035804e-01,   2.08240681e-01,\n",
      "         9.41968781e-01,   9.85113988e-01,   1.05253960e+00,\n",
      "         7.11584653e-01,   8.21610519e-02,   1.41729678e-01,\n",
      "         5.99023436e-01,   7.22978278e-01,   1.44188828e-01,\n",
      "         1.00184390e+00,   9.19563065e-01,   1.02888937e-01,\n",
      "         8.57054803e-02,   7.73697958e-01,   7.28264775e-01,\n",
      "        -1.54666585e-01,   9.86352133e-01,  -3.61883319e-02,\n",
      "         7.16962630e-01,   5.40204436e-01,   9.88161853e-01,\n",
      "         6.45309190e-01,   3.35502622e-01,   4.62016785e-01,\n",
      "         3.83999487e-02,   1.04693048e+00,   1.81405046e-01,\n",
      "         3.78405011e-01,   9.67684665e-01,   2.20343935e-02,\n",
      "         3.43113788e-01,   2.86842707e-01,   9.66427632e-01,\n",
      "         2.35509830e-01,   2.46143115e-01,   2.07154403e-01,\n",
      "         7.59495771e-01,   7.02784917e-01,   6.50695432e-01,\n",
      "         1.25782324e-01,   3.31785869e-02,   1.00991927e-01,\n",
      "         5.02195997e-01,   9.40220300e-02,   5.36945264e-02,\n",
      "         1.98850981e-01,   8.34991729e-02,   9.46404752e-01,\n",
      "         7.09873774e-01,   7.27210373e-01,   7.27210373e-01,\n",
      "        -8.26619864e-03,   2.14888763e-01,   5.50885299e-01,\n",
      "         5.66445411e-02,   1.76397669e-01,   7.45335102e-02,\n",
      "         8.03556708e-01,   7.08662712e-01,   7.27102602e-01,\n",
      "         9.73554281e-01,   4.01280658e-01,   6.90762849e-02,\n",
      "         1.14189531e-01,   5.26165903e-01,   5.45184685e-01,\n",
      "         1.02199530e+00,   6.12697165e-01,   5.63117369e-01,\n",
      "         1.25282751e-01,   1.13960144e-01,   1.04173574e+00,\n",
      "         7.03609329e-01,   6.80076952e-02,   8.54768283e-01,\n",
      "         1.81405046e-01,   3.21419341e-01,  -3.50204130e-02,\n",
      "         7.16962630e-01,   2.17277025e-01,   8.01478106e-01,\n",
      "         3.89527338e-01,   1.06888637e-01,  -1.25371118e-02,\n",
      "         9.66418579e-01,   6.04881357e-01,   1.03344209e-01,\n",
      "         5.81301294e-01,   1.82553887e-01,   2.51146632e-01,\n",
      "         7.45026110e-01,   4.31966967e-02,   8.93785324e-02,\n",
      "         5.71028233e-01,   4.14346762e-02,   6.20964239e-01,\n",
      "         1.73555087e-01,   5.17582468e-04,   2.77250792e-01,\n",
      "         1.06871287e-01,   5.59481245e-01,   1.81405046e-01,\n",
      "         2.16550576e-01,   9.29565253e-01,   3.23910056e-01,\n",
      "         2.54745548e-02,   6.91341898e-01,   6.88651110e-01,\n",
      "         7.72152804e-01,   2.28420973e-01,   6.60717667e-01,\n",
      "         1.99410519e-01,   1.42847449e-01,   7.85992735e-02,\n",
      "         5.45728638e-01,   7.62131098e-02,   1.80866361e-01,\n",
      "         7.08800565e-01,   7.88061680e-01,   2.17294375e-01,\n",
      "         6.81119622e-02,   3.95903553e-01,   6.50695432e-01,\n",
      "         6.59178568e-01,   1.20506812e-01,   2.95046760e-01,\n",
      "         9.77312989e-01,   5.35535475e-01,   6.56655227e-01,\n",
      "         4.00955254e-01,   2.22791205e-01,   6.44449048e-01,\n",
      "         1.11854600e-01,   4.53323131e-02,   7.45318057e-01,\n",
      "         1.81585803e-01,   5.18494941e-01,   7.99601598e-01,\n",
      "         3.74636983e-01,   6.01947186e-01,   3.14132110e-01,\n",
      "         1.63487112e-01,   1.44112558e-01,   4.07821133e-01,\n",
      "         3.44293480e-01,   1.81533670e-01,   9.66220964e-02,\n",
      "         2.64759704e-01,   9.78708582e-01,   6.65255293e-01,\n",
      "         2.17294375e-01,   3.25014986e-01,   6.10231055e-02,\n",
      "         3.38527339e-01,   1.64621262e-01,   1.81533670e-01,\n",
      "         4.60112560e-02,   3.23910056e-01,   2.96659559e-01,\n",
      "         2.17273521e-01,   6.17011007e-01,   1.80866361e-01,\n",
      "         4.73563003e-02,   6.83928625e-01,   7.81706057e-01,\n",
      "         6.14855492e-01,   5.16376308e-01,   1.73555087e-01,\n",
      "         3.05463477e-02,   1.03114821e-01,   6.83482924e-01,\n",
      "        -6.33758399e-02,   3.23910056e-01,   4.31225163e-03,\n",
      "         4.69325395e-01,   4.31007179e-01,   5.59481245e-01,\n",
      "         8.86465451e-01,   3.10203703e-01,   7.52008190e-02,\n",
      "         1.04515754e-01,   1.44112558e-01,   1.06433365e-01,\n",
      "         3.03498825e-01,   2.80645531e-01,   1.07100674e-01,\n",
      "         2.04884536e-01,   8.27339617e-01,   1.87414329e-01,\n",
      "         9.64641268e-01,   9.62553518e-02,   1.21789296e-01,\n",
      "         6.52669607e-01,   7.27001839e-01,   5.61480532e-01,\n",
      "         9.98251374e-01,   4.70683148e-01,   7.29830147e-01,\n",
      "         4.95147548e-01,   9.43637942e-02,   1.29788918e-01,\n",
      "         1.14505952e-01,   1.81533670e-01,   3.67329166e-01,\n",
      "         8.11602223e-01,   9.60051110e-02,   3.34791077e-01,\n",
      "         7.21966147e-01,   2.30974205e-01,   6.80445138e-01,\n",
      "         6.79833385e-02,   9.54468329e-01,   1.03427622e-01,\n",
      "         1.98850981e-01,   8.88917858e-01,   1.98854484e-01,\n",
      "         7.37450260e-02,   6.65255293e-01,   6.57995250e-01,\n",
      "         4.31966967e-02,   1.52953479e-01,   7.96650934e-01,\n",
      "         1.98854484e-01,   8.69331732e-02,   7.12672902e-01,\n",
      "         5.85519955e-01,   8.22193818e-01,   3.35648271e-01,\n",
      "         9.83255009e-01,   1.86913847e-01,   9.55517708e-01,\n",
      "         9.05824162e-01,   4.80760273e-01,   4.73671416e-01,\n",
      "         1.84814624e-01,   3.55196954e-01,   2.67845357e-01,\n",
      "         7.40700420e-01,   3.53478739e-01,   2.33073518e-02,\n",
      "         2.74543098e-01,   5.14633642e-01,   2.49898026e-01,\n",
      "         2.17360438e-01,   1.20873557e-01,   6.13135086e-01,\n",
      "         2.75392989e-01,   8.16352901e-01,   6.80793501e-01,\n",
      "         8.59349661e-01,   5.24180388e-01,   2.17294375e-01,\n",
      "         3.96279115e-02,   2.35926898e-01,   1.81533670e-01,\n",
      "         6.91341898e-01,   1.51744581e-02,   1.14060907e-01,\n",
      "         5.82199023e-01,   1.98854484e-01,   6.79486384e-02,\n",
      "         1.12913925e-01,   7.43172894e-01,   3.67412579e-01,\n",
      "         7.27102602e-01,   1.24822816e-01,   1.19569708e-01,\n",
      "         7.24051487e-01,   8.43843700e-01,   6.67331504e-01,\n",
      "         6.80076952e-02,   7.18465626e-01,   9.03407954e-01,\n",
      "         1.54198460e-01,   3.96876995e-01,   2.00091758e-01,\n",
      "         9.76839149e-01,   1.69833963e-01,   2.92738550e-01,\n",
      "         1.03556246e-01,   1.81533670e-01,   5.67071013e-02,\n",
      "         8.34428473e-01,   1.42782279e-02,   7.18493066e-01]), array([ 0.18064678,  0.04707049,  0.82291749,  0.04782057,  0.17889199,\n",
      "        0.333026  ,  0.7178753 ,  0.14578577,  0.44140942,  0.05579373,\n",
      "        0.40720478,  0.03297946,  0.08445692,  0.38953706,  0.80437774,\n",
      "        0.86013147,  0.63643239,  0.146022  ,  0.70393872,  0.21166423,\n",
      "        0.07419238,  0.76082899,  0.0538822 ,  0.58478174,  0.75540967,\n",
      "        0.244537  ,  0.08460304,  0.31563911,  0.11504646,  0.1028672 ,\n",
      "        0.10056419,  0.31952249,  0.2464849 ,  0.92006094,  0.0926341 ,\n",
      "        0.21166889,  0.28132907,  0.04176178,  0.41223445,  0.45325149,\n",
      "        0.57507716,  0.77126311,  0.08036018,  0.20933568,  0.66706816,\n",
      "        0.10642047,  0.10491149,  0.94926514,  0.58938722,  0.13221208,\n",
      "        0.647856  ,  0.32954027,  0.10694417,  0.3617749 ,  0.14588235,\n",
      "        0.63037651,  0.14578577,  0.80540608,  0.10270546,  0.74315505,\n",
      "        0.70665915,  0.15688223,  0.14578577,  0.65501424,  0.24997165,\n",
      "        0.36895012,  0.23956126,  0.09712019,  0.35002319,  0.07230287,\n",
      "        0.0678178 ,  0.14052787,  0.24588306,  0.14581951,  0.05783802,\n",
      "        0.93316722,  0.71888897,  0.33828114,  0.15458802,  0.23361729,\n",
      "        0.30062125,  0.1090222 ,  0.09565748,  0.63995792,  0.6403842 ,\n",
      "        0.28597819,  0.74306894,  0.40954438,  0.10452863, -0.04611168,\n",
      "        0.14649013,  0.26673192, -0.02351631,  0.10630877,  0.10901639,\n",
      "        0.91156765,  0.33688467,  0.7169914 ,  0.13221208,  0.12778462,\n",
      "        0.32317854,  0.19054745,  0.05579373,  0.74318183,  0.33444989,\n",
      "        0.05999403,  0.88026569,  0.37435922,  0.6880815 ,  0.16055917,\n",
      "        0.0555718 ,  0.21363211,  0.73637045,  0.36004916,  0.89746292,\n",
      "        0.15607086,  0.95479399,  0.44583623,  0.30352812,  0.08605784,\n",
      "        0.1906871 ,  0.10701282,  0.87645371,  0.77400564,  0.2094997 ,\n",
      "        0.07835778,  0.82714467,  0.07419389,  0.2274844 ,  0.19477569,\n",
      "        0.34615109,  0.10468574,  0.65371842,  0.74318066,  0.20517832,\n",
      "        0.58910927,  0.86645457,  0.15317541,  0.30062125,  0.24997165,\n",
      "        0.24997165,  0.08523198,  0.58418202,  0.27152953,  0.14578577,\n",
      "        0.14578577,  0.44487267,  0.32056062,  0.8603114 ,  0.05735739,\n",
      "        0.08240447,  0.26974   ,  0.08904563,  0.76717452,  0.36250545,\n",
      "        0.14829109,  0.79695229,  0.14873401,  0.07787599,  0.09876699,\n",
      "        0.71176592,  0.27462085,  0.08857344,  0.26061206,  0.07623668,\n",
      "        0.89314376,  0.14394126,  0.06218496,  0.13800492,  0.87356651,\n",
      "        0.11311079,  0.82165818,  0.47511031,  0.68082617,  0.17000989,\n",
      "        0.08023799,  0.09627465,  0.04767123,  0.7126768 ,  0.17889199,\n",
      "        0.72772345,  0.10902918,  0.21166423,  0.66579829,  0.21166772,\n",
      "        0.85432739,  0.68382792,  0.89701774,  0.38558811,  0.13735356,\n",
      "        0.09491462,  0.09485877,  0.64061312,  0.05753724,  0.13611224,\n",
      "        0.3700127 ,  0.21166423,  0.26065089,  0.39647363,  0.4775972 ,\n",
      "        0.0946784 ,  0.21726293,  0.87735211,  0.61370562,  0.11756197,\n",
      "        0.5908492 ,  0.2274844 ,  0.74084862,  0.39316095,  0.19416557,\n",
      "        0.09084698,  0.08244637,  0.37352399,  0.64053514,  0.21726293,\n",
      "        0.86539826,  0.09263642,  0.06151857,  0.22613834,  0.54319351,\n",
      "        0.08339171,  0.45325149,  0.63027177,  0.23925096,  0.05828489,\n",
      "        0.04151992,  0.83176614,  0.09037567,  0.37556828,  0.63731676,\n",
      "        0.00101523,  0.21188766,  0.13221208,  0.35414668,  0.21166423,\n",
      "        0.83567462,  0.64064752,  0.27310676,  0.17889082,  0.09877861,\n",
      "        0.10887557,  0.89414921,  0.10264144,  0.1457427 ,  0.06679464,\n",
      "        0.47787532,  0.10489172,  0.26497995,  0.94769633,  0.10858376,\n",
      "        0.11081862,  0.06002196, -0.13503931,  0.10715432,  0.25065725,\n",
      "        0.9557231 ,  0.01576916, -0.00545831,  0.66918515,  0.95754448,\n",
      "        0.69784966,  0.65658821,  0.8261439 ,  0.34157719,  0.70745988,\n",
      "        0.17889082, -0.01391327,  0.21740526,  0.8793964 ,  0.39647363,\n",
      "        0.24792735,  0.74264299,  0.76891264,  0.37577517,  0.14533773,\n",
      "        0.11612341,  0.0926341 ,  0.85342353,  0.38439329,  0.04939551,\n",
      "        0.76403583,  0.67974051,  0.10435525,  0.10694417,  0.14578577,\n",
      "        0.89647012,  0.79025813,  0.07832404,  0.63159222,  0.24044841,\n",
      "        0.09491462,  0.73994453,  0.24179447,  0.937544  ,  0.66886651,\n",
      "        0.42463987,  0.14624679])]\n"
     ]
    }
   ],
   "source": [
    "# Import linear regression and cross validation from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "titanic['Age'] = titanic['Age'].fillna(0)\n",
    "titanic['SibSp'] = titanic['SibSp'].fillna(0)\n",
    "titanic['Parch'] = titanic['Parch'].fillna(0)\n",
    "titanic['Fare'] = titanic['Fare'].fillna(0)\n",
    "\n",
    "# Columns we use for predicting the target\n",
    "predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "\n",
    "# Initialize the algorithm\n",
    "alg = LinearRegression()\n",
    "\n",
    "# Generate Cross validation folds for the titanic dataset.\n",
    "kf = KFold(titanic.shape[0], n_folds = 3, random_state = 1)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for train, test in kf:\n",
    "    # we only take the rows in the train fold\n",
    "    train_predictors = titanic[predictors].iloc[train,:]\n",
    "    # The target we use to train the algorithm\n",
    "    train_target = titanic['Survived'].iloc[train]\n",
    "    # Training the algorithm using the predictors and target\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make the predictions on test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)\n",
    "    \n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now evaluate the errors, the metrics will basically ivolve finding the number of values in predictions that are exactly same as their counterparts in titanic['Survived'] and then dividing by the total number of passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy percentage is 77.8900112233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vanshdeep Sharma\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "''' The predictions are in three different numpy arrays, we concatenate them into one.\n",
    "We concatenate them on axis 0, as they only have one axis '''\n",
    "\n",
    "predictions = np.concatenate(predictions, axis = 0)\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions < .5] = 0\n",
    "\n",
    "accuracy = sum(predictions[predictions == titanic['Survived']]) / len(predictions)\n",
    "i = accuracy * 100\n",
    "print('Accuracy percentage is %s' %i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have our accuracy percentage, but it's not so good, it's only 77.89% accurate. We can instead use logistic regression, it takes the value output of a linear regression and maps it to a probability value between 0 and 1. \n",
    "- The mapping is done using the logit function, passing any function through the logit function will map it to a value between 0 and 1 by 'squeezing' the extreme values.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00793610304362\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state = 1)\n",
    "\n",
    "# Compute accuracy score for all cross validation folds\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic['Survived'], cv = 3)\n",
    "\n",
    "# Take the mean of the scores\n",
    "Scores = scores * 100\n",
    "print(scores.std())\n",
    "# print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we make all the changes to the test file as we did to the training data\n",
    "titanic_test = pd.read_csv('d:/titanic_test.csv')\n",
    "titanic_test['Age'] = titanic_test['Age'].fillna(titanic['Age'].median())\n",
    "\n",
    "titanic_test.loc[titanic['Sex'] == 'male', 'Sex'] = 0\n",
    "titanic_test.loc[titanic['Sex'] == 'female', 'Sex'] = 1\n",
    "\n",
    "titanic_test['Embarked'] = titanic_test['Embarked'].fillna('S')\n",
    "titanic_test.loc[titanic_test['Embarked'] == 'S', 'Embarked'] = 0\n",
    "titanic_test.loc[titanic_test['Embarked'] == 'C', 'Embarked'] = 1\n",
    "titanic_test.loc[titanic_test['Embarked'] == 'Q', 'Embarked'] = 2\n",
    "\n",
    "titanic_test['Fare'] = titanic_test['Fare'].fillna(titanic['Fare'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest \n",
    "- Here we build hundreds of trees with slightly randomized input data, and slightly randomized split points. Each tree in an random forest gets a random subset of the overall training data. \n",
    "- Each split point in each tree is performed on a random subset of the potential columns to split on. By averaging the predictions of all the trees, we get a stronger overall prediction and minimize overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80022446689113347"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "\n",
    "# Initialize the algorithm with the default values\n",
    "# n_estimators is the number of tree we want to make\n",
    "# min_samples_split is the minimun number of row we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends\n",
    "\n",
    "alg = RandomForestClassifier(random_state = 1, n_estimators = 10, \n",
    "                            min_samples_split = 2, min_samples_leaf = 1)\n",
    "kf = KFold(titanic.shape[0], n_folds = 3, random_state = 1)\n",
    "\n",
    "scores = cross_val_score(alg, titanic[predictors], titanic['Survived'], cv = kf)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An easy way to generate features is to use the .apply() on pandas dataframes. This applies a function you pass in to each element in a dataframe or a  series. \n",
    "- To write a lambda function, you write lambda x: len(x). x will take on the value of the input that is passed in -- in this case the ticket length. The function to the right of the colon is then applied to x and the result is returned. \n",
    "- The .apply method takes all the outputs and constructs a pandas series from them. We can assign this series to a dataframe series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating  a familysize column\n",
    "titanic['FamilySize'] = titanic['SibSp'] + titanic['Parch']\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic['TicketLength'] = titanic['Ticket'].apply(lambda x:len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       0\n",
       "3       1\n",
       "4       0\n",
       "5       0\n",
       "6       0\n",
       "7       4\n",
       "8       2\n",
       "9       1\n",
       "10      2\n",
       "11      0\n",
       "12      0\n",
       "13      6\n",
       "14      0\n",
       "15      0\n",
       "16      5\n",
       "17      0\n",
       "18      1\n",
       "19      0\n",
       "20      0\n",
       "21      0\n",
       "22      0\n",
       "23      0\n",
       "24      4\n",
       "25      6\n",
       "26      0\n",
       "27      5\n",
       "28      0\n",
       "29      0\n",
       "       ..\n",
       "861     1\n",
       "862     0\n",
       "863    10\n",
       "864     0\n",
       "865     0\n",
       "866     1\n",
       "867     0\n",
       "868     0\n",
       "869     2\n",
       "870     0\n",
       "871     2\n",
       "872     0\n",
       "873     0\n",
       "874     1\n",
       "875     0\n",
       "876     0\n",
       "877     0\n",
       "878     0\n",
       "879     1\n",
       "880     1\n",
       "881     0\n",
       "882     0\n",
       "883     0\n",
       "884     0\n",
       "885     5\n",
       "886     0\n",
       "887     0\n",
       "888     3\n",
       "889     0\n",
       "890     0\n",
       "Name: FamilySize, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['FamilySize']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also extract the title of the passengers from their names. There are very few commonly used titles, and a 'long tail' of one-off titles that only one or two passengers have. \n",
    "- We first extract all titles using regular expression and then map each unique title to an integer value. We'll then have a numeric column that corresponds to the appropriate Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Major         2\n",
      "Col           2\n",
      "Mlle          2\n",
      "Countess      1\n",
      "Capt          1\n",
      "Sir           1\n",
      "Lady          1\n",
      "Don           1\n",
      "Mme           1\n",
      "Ms            1\n",
      "Jonkheer      1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_title(name):\n",
    "    # Use regular expression to search for title\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If title exists extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all titles and print how often they occur\n",
    "titles = titanic['Name'].apply(get_title)\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer. Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Dr': 5,\n",
    "                 'Rev': 6, 'Major': 7, 'Col': 7, 'Mlle': 8, 'Mme': 8,\n",
    "                 'Don': 9, 'Lady': 10, 'Countess': 10, 'Jonkheer': 10,\n",
    "                 'Sir': 9, 'Capt': 7, 'Ms': 2}\n",
    "\n",
    "for k, v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Add in the title column\n",
    "titanic['Title'] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1    800\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the id from a row\n",
    "def get_family_id(row):\n",
    "    # Find the last name\n",
    "    last_name = row['Name'].split(',')[0]\n",
    "    # Create the family id\n",
    "    family_id = '{0}{1}'.format(last_name, row['FamilySize'])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), \n",
    "                              key = operator.itemgetter(1))[1] + 1)\n",
    "            family_id_mapping[family_id] = current_id\n",
    "       # return family_id_mapping[family_id]\n",
    "\n",
    "# Get the family ids with the apply method\n",
    "family_ids = titanic.apply(get_family_id, axis = 1)\n",
    "\n",
    "# Compress all the families under 3 members into one code\n",
    "family_ids[titanic['FamilySize'] < 3] = -1\n",
    "\n",
    "# Print the count of each unique id\n",
    "print(pd.value_counts(family_ids))\n",
    "\n",
    "titanic['FamilyId'] = family_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAGhCAYAAACEQgY6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e240df9a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.81930415263748602"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn has a function that helps us with feature selection, SelectKBest. This selects the best features from the data, and allows us to specify how many it selects\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "titanic['FamilyId'] = titanic['FamilyId'].fillna(0)\n",
    "predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked',\n",
    "              'FamilySize', 'Title', 'FamilyId']\n",
    "\n",
    "# perform feature selection\n",
    "selector = SelectKBest(f_classif, k = 5)\n",
    "selector.fit(titanic[predictors], titanic['Survived'])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores, see how 'Pclass', 'Sex', 'Title' and 'Fare' are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation = 'vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best\n",
    "predictors = ['Pclass', 'Sex', 'Fare', 'Title']\n",
    "\n",
    "alg = RandomForestClassifier(random_state = 1, n_estimators = 50,\n",
    "                             min_samples_split = 8, min_samples_leaf = 4)\n",
    "kf = KFold(titanic.shape[0], n_folds = 3, random_state = 1)\n",
    "scores = cross_val_score(alg, titanic[predictors], titanic['Survived'], cv = kf)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "- It involves training decision trees one after the other and feeding the errors from one tree into the next tree. \n",
    "- So each tree is building on all the other trees that came before it. We'll try boosing instead of out Random Forest approach and see if we can improve our accuracy, we'll limit the tree depth to 3 to avoid overfitting.\n",
    "\n",
    "### Ensembling\n",
    "- Here we generate predictions using information from a set of classifiers, instead of just one, in practice this means we average their predictions.\n",
    "- In this case we'll ensemble logistic regression trained on the most linear predictors and a gradient boosted tree trained on all of the predictors.\n",
    "- We'll keep things simple when we ensemble -- we'll average the raw probabilities from 0 to 1 that we get from our classifiers and then assume anything above .5 maps to one and anything below .5 maps to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pclass', 'Sex', 'Fare', 'FamilySize', 'Title', 'Age', 'Embarked', array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble\n",
    "# We'll use linear predictors from logistic regression and everything with gradient boosting classifier\n",
    "\n",
    "algorithms = [[GradientBoostingClassifier(random_state = 1, n_estimators = 2,\n",
    "                                          max_depth = 3), ['Pclass', 'Sex', 'Age',\n",
    "                                                           'Fare', 'Embarked', 'FamilySize',\n",
    "                                                           'Title', 'FamilyId']],\n",
    "              [LogisticRegression(random_state = 1), ['Pclass', 'Sex', 'Fare',\n",
    "                                                                'FamilySize', 'Title',\n",
    "                                                                'Age', 'Embarked']]]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds = 3, random_state = 1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic['Survived'].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictions in algorithms:\n",
    "        # Fit the algorithm on the training data\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # select and predict on test fold\n",
    "        # the astype(float) is necessary to convert the dataframe to all float and avoid sklearn error\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "        # Use a simple ensembling scheme -- just average the predictions to get the final classifications\n",
    "        test_predictions = (full_test_predictions[0]) / 2\n",
    "        # Any values above .5 are assumed 1 and below .5 are assumed 0\n",
    "        test_predictions[test_predictions <= .5] = 0\n",
    "        test_predictions[test_predictions > .5] = 1\n",
    "        predictions.append(test_predictions)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we make all the change to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      240\n",
      "2       79\n",
      "3       72\n",
      "4       21\n",
      "Col      2\n",
      "6        2\n",
      "10       1\n",
      "5        1\n",
      "Name: Title, dtype: int64\n",
      "0        -1\n",
      "1        -1\n",
      "2        -1\n",
      "3        -1\n",
      "4        -1\n",
      "5        -1\n",
      "6        -1\n",
      "7        -1\n",
      "8        -1\n",
      "9        -1\n",
      "10       -1\n",
      "11       -1\n",
      "12       -1\n",
      "13       -1\n",
      "14       -1\n",
      "15       -1\n",
      "16       -1\n",
      "17       -1\n",
      "18       -1\n",
      "19       -1\n",
      "20       -1\n",
      "21       -1\n",
      "22       -1\n",
      "23       -1\n",
      "24     None\n",
      "25       -1\n",
      "26       -1\n",
      "27       -1\n",
      "28       -1\n",
      "29       -1\n",
      "       ... \n",
      "388      -1\n",
      "389    None\n",
      "390      -1\n",
      "391      -1\n",
      "392      -1\n",
      "393      -1\n",
      "394    None\n",
      "395      -1\n",
      "396      -1\n",
      "397      -1\n",
      "398      -1\n",
      "399      -1\n",
      "400      -1\n",
      "401      -1\n",
      "402      -1\n",
      "403      -1\n",
      "404      -1\n",
      "405      -1\n",
      "406      -1\n",
      "407      -1\n",
      "408      -1\n",
      "409      -1\n",
      "410      -1\n",
      "411      -1\n",
      "412      -1\n",
      "413      -1\n",
      "414      -1\n",
      "415      -1\n",
      "416      -1\n",
      "417      -1\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "titles = titanic_test['Name'].apply(get_title)\n",
    "\n",
    "title_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4,\n",
    "                'Dr': 5, 'Rev': 6, 'Major': 7, 'Mlle': 8, \n",
    "                'Mme': 8, 'Don': 9, 'Lady': 10, 'Countess': 10,\n",
    "                'Jonkheer': 10, 'Sir': 9, 'Capt': 7, 'Ms': 2,\n",
    "                'Dona': 10}\n",
    "for k, v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test['Title'] = titles\n",
    "\n",
    "print(pd.value_counts(titanic_test['Title']))\n",
    "\n",
    "titanic_test['FamilySize'] = titanic_test['SibSp'] + titanic_test['Parch']\n",
    "\n",
    "print(family_ids)\n",
    "\n",
    "family_ids = titanic_test.apply(get_family_id, axis = 1)\n",
    "family_ids[titanic_test['FamilySize'] < 3] = -1\n",
    "titanic_test['FamilyId'] = family_ids\n",
    "titanic_test['NameLength'] = titanic_test['Name'].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the analysis let's now make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         0\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         0\n",
      "5            897         0\n",
      "6            898         0\n",
      "7            899         1\n",
      "8            900         0\n",
      "9            901         0\n",
      "10           902         0\n",
      "11           903         0\n",
      "12           904         1\n",
      "13           905         0\n",
      "14           906         1\n",
      "15           907         1\n",
      "16           908         0\n",
      "17           909         0\n",
      "18           910         0\n",
      "19           911         0\n",
      "20           912         1\n",
      "21           913         0\n",
      "22           914         1\n",
      "23           915         1\n",
      "24           916         1\n",
      "25           917         0\n",
      "26           918         1\n",
      "27           919         0\n",
      "28           920         1\n",
      "29           921         0\n",
      "..           ...       ...\n",
      "388         1280         0\n",
      "389         1281         0\n",
      "390         1282         1\n",
      "391         1283         1\n",
      "392         1284         0\n",
      "393         1285         0\n",
      "394         1286         0\n",
      "395         1287         1\n",
      "396         1288         0\n",
      "397         1289         1\n",
      "398         1290         0\n",
      "399         1291         0\n",
      "400         1292         1\n",
      "401         1293         0\n",
      "402         1294         1\n",
      "403         1295         0\n",
      "404         1296         1\n",
      "405         1297         0\n",
      "406         1298         0\n",
      "407         1299         1\n",
      "408         1300         0\n",
      "409         1301         0\n",
      "410         1302         0\n",
      "411         1303         1\n",
      "412         1304         0\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         0\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "titanic_test['FamilySize'] = titanic_test['FamilySize'].fillna(0)\n",
    "titanic_test['FamilyId'] = titanic_test['FamilyId'].fillna(0)\n",
    "\n",
    "predictors = ['Pclass', 'Age', 'Fare', 'Embarked', 'FamilySize',\n",
    "              'FamilyId']\n",
    "\n",
    "algorithm = [\n",
    "    [GradientBoostingClassifier(random_state = 1, n_estimators = 25, max_depth = 3),\n",
    "     predictors],\n",
    "[LogisticRegression(random_state = 1), ['Pclass', 'Fare', 'FamilySize', \n",
    "                                       'Age', 'Embarked']]\n",
    "    ]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithm:\n",
    "    # fit the algorithm using the full training data\n",
    "    alg.fit(titanic[predictors], titanic['Survived'])\n",
    "    # Predict using the test dataset. We have to convert all the columns to floats to avoid an error\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "predictions[predictions <= .5] = 0\n",
    "predictions[predictions > .5] = 1\n",
    "predictions = predictions.astype(int)\n",
    "submission = pd.DataFrame({'PassengerId': titanic_test['PassengerId'],\n",
    "                          'Survived': predictions})\n",
    "submission.to_csv('Kaggle.csv', index = False)\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
